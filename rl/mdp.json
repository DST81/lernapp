[
  {
    "question": "Welche Komponenten definieren formal einen Markov Decision Process (MDP)?",
    "options": [
      "$$(S, A, R)$$",
      "$$(S, A, P, R, \\gamma)$$",
      "$$(S, A, P, \\pi)$$",
      "$$(S, A, R, V)$$"
    ],
    "correct_index": 1,
    "explanation": "Ein MDP wird definiert durch Zustände $$S$$, Aktionen $$A$$, Übergangswahrscheinlichkeiten $$P$$, Reward-Funktion $$R$$ und Diskontfaktor $$\\gamma$$."
  },
  {
    "question": "Was besagt die Markov-Eigenschaft eines MDPs?",
    "options": [
      "Der Reward hängt nur vom aktuellen Zustand ab",
      "Der nächste Zustand hängt nur vom aktuellen Zustand und der Aktion ab",
      "Die Policy ist zeitinvariant",
      "Der Return ist zeitdiskontiert"
    ],
    "correct_index": 1,
    "explanation": "Die Markov-Eigenschaft fordert, dass die Zukunft unabhängig von der Vergangenheit ist, gegeben den aktuellen Zustand und die Aktion."
  },
  {
    "question": "Wie ist der Return $$G_t$$ in einem episodischen MDP definiert?",
    "options": [
      "$$G_t = \\sum_{k=0}^{\\infty} R_{t+k+1}$$",
      "$$G_t = \\sum_{k=0}^{T-t-1} R_{t+k+1}$$",
      "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$",
      "$$G_t = R_{t+1}$$"
    ],
    "correct_index": 2,
    "explanation": "Der Return ist die Summe zukünftiger Rewards, diskontiert mit $$\\gamma$$."
  },
  {
    "question": "Welche Rolle spielt der Diskontfaktor $$\\gamma \\in [0,1]$$?",
    "options": [
      "Er begrenzt die Anzahl der Zustände",
      "Er kontrolliert die Varianz der Rewards",
      "Er gewichtet zukünftige Belohnungen",
      "Er bestimmt die Lernrate"
    ],
    "correct_index": 2,
    "explanation": "Der Diskontfaktor legt fest, wie stark zukünftige Rewards im Return berücksichtigt werden."
  },
  {
    "question": "Welche Interpretation hat $$\\gamma = 0$$?",
    "options": [
      "Alle zukünftigen Rewards zählen gleich stark",
      "Nur der unmittelbare Reward zählt",
      "Der Agent maximiert langfristige Belohnung",
      "Der Return divergiert"
    ],
    "correct_index": 1,
    "explanation": "Bei $$\\gamma = 0$$ berücksichtigt der Return nur den nächsten Reward."
  },
  {
    "question": "Welche Aussage zu $$\\gamma = 1$$ ist korrekt?",
    "options": [
      "Der Return ist immer endlich",
      "Nur episodische MDPs sind zulässig",
      "Der Agent ignoriert zukünftige Rewards",
      "Bootstrapping ist nicht möglich"
    ],
    "correct_index": 1,
    "explanation": "Bei $$\\gamma = 1$$ muss das MDP episodisch sein, damit der Return endlich bleibt."
  },
  {
    "question": "Wie ist die Zustandswertfunktion unter einer Policy $$\\pi$$ definiert?",
    "options": [
      "$$V_\\pi(s) = \\mathbb{E}[R_{t+1} \\mid S_t=s]$$",
      "$$V_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t=s]$$",
      "$$V_\\pi(s) = \\max_a Q_\\pi(s,a)$$",
      "$$V_\\pi(s) = \\sum_a \\pi(a|s)$$"
    ],
    "correct_index": 1,
    "explanation": "Die Value-Funktion ist der erwartete Return bei Start in Zustand $$s$$ und Befolgung der Policy $$\\pi$$."
  },
  {
    "question": "Wie ist die Aktionswertfunktion $$Q_\\pi(s,a)$$ definiert?",
    "options": [
      "$$Q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t=s, A_t=a]$$",
      "$$Q_\\pi(s,a) = R(s,a)$$",
      "$$Q_\\pi(s,a) = V_\\pi(s) + R(s,a)$$",
      "$$Q_\\pi(s,a) = \\max_s V_\\pi(s)$$"
    ],
    "correct_index": 0,
    "explanation": "Q-Werte geben den erwarteten Return an, wenn in Zustand $$s$$ Aktion $$a$$ gewählt wird."
  },
  {
    "question": "Welche Beziehung gilt zwischen $$V_\\pi$$ und $$Q_\\pi$$?",
    "options": [
      "$$V_\\pi(s) = \\max_a Q_\\pi(s,a)$$",
      "$$V_\\pi(s) = \\sum_a \\pi(a|s) Q_\\pi(s,a)$$",
      "$$Q_\\pi(s,a) = R(s)$$",
      "$$V_\\pi(s) = Q_\\pi(s,\\pi(s))$$"
    ],
    "correct_index": 1,
    "explanation": "Der Zustandswert ist der Erwartungswert der Aktionswerte unter der Policy."
  },
  {
    "question": "Welche Gleichung beschreibt die Bellman-Erwartungsgleichung für $$V_\\pi(s)$$?",
    "options": [
      "$$V_\\pi(s) = \\max_a \\sum_{s'} P(s'|s,a) [R + \\gamma V_\\pi(s')]$$",
      "$$V_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) [R + \\gamma V_\\pi(s')]$$",
      "$$V_\\pi(s) = R(s)$$",
      "$$V_\\pi(s) = \\sum_{s'} V_\\pi(s')$$"
    ],
    "correct_index": 1,
    "explanation": "Die Bellman-Erwartungsgleichung zerlegt den Value in unmittelbaren Reward und erwarteten Folgewert."
  },
  {
    "question": "Was unterscheidet die Bellman-Optimalitätsgleichung von der Erwartungsgleichung?",
    "options": [
      "Sie nutzt Bootstrapping",
      "Sie ersetzt die Policy-Erwartung durch ein Maximum über Aktionen",
      "Sie gilt nur für Q-Funktionen",
      "Sie ignoriert Übergangswahrscheinlichkeiten"
    ],
    "correct_index": 1,
    "explanation": "In der Optimalitätsgleichung wird über alle Aktionen maximiert."
  },
  {
    "question": "Welche Definition trifft auf die optimale Zustandswertfunktion $$V^*(s)$$ zu?",
    "options": [
      "$$V^*(s) = \\max_\\pi V_\\pi(s)$$",
      "$$V^*(s) = \\mathbb{E}[R_{t+1}]$$",
      "$$V^*(s) = \\sum_a Q_\\pi(s,a)$$",
      "$$V^*(s) = R(s)$$"
    ],
    "correct_index": 0,
    "explanation": "Die optimale Value-Funktion ist der maximale erreichbare erwartete Return über alle Policies."
  },
  {
    "question": "Welche Eigenschaft garantiert die Existenz einer optimalen deterministischen Policy?",
    "options": [
      "Stetige Zustandsräume",
      "Endliche Zustands- und Aktionsräume",
      "Stochastische Rewards",
      "Nicht-Stationarität"
    ],
    "correct_index": 1,
    "explanation": "Für endliche MDPs existiert immer mindestens eine optimale deterministische Policy."
  }
]
