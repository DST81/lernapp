[
  {
    "question": "Welche Komponenten definieren formal einen Markov Decision Process (MDP)?",
    "options": [
      "$$(S, A, R)$$",
      "$$(S, A, P, R, \\gamma)$$",
      "$$(S, A, P, \\pi)$$",
      "$$(S, A, R, V)$$"
    ],
    "correct_index": 1,
    "explanation": "Ein MDP wird definiert durch Zustände $$S$$, Aktionen $$A$$, Übergangswahrscheinlichkeiten $$P$$, Reward-Funktion $$R$$ und Diskontfaktor $$\\gamma$$."
  },
  {
    "question": "Was besagt die Markov-Eigenschaft eines MDPs?",
    "options": [
      "Der Reward hängt nur vom aktuellen Zustand ab",
      "Der nächste Zustand hängt nur vom aktuellen Zustand und der Aktion ab",
      "Die Policy ist zeitinvariant",
      "Der Return ist zeitdiskontiert"
    ],
    "correct_index": 1,
    "explanation": "Die Markov-Eigenschaft fordert, dass die Zukunft unabhängig von der Vergangenheit ist, gegeben den aktuellen Zustand und die Aktion."
  },
  {
    "question": "Wie ist der Return $$G_t$$ in einem episodischen MDP definiert?",
    "options": [
      "$$G_t = \\sum_{k=0}^{\\infty} R_{t+k+1}$$",
      "$$G_t = \\sum_{k=0}^{T-t-1} R_{t+k+1}$$",
      "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$",
      "$$G_t = R_{t+1}$$"
    ],
    "correct_index": 2,
    "explanation": "Der Return ist die Summe zukünftiger Rewards, diskontiert mit $$\\gamma$$."
  },
  {
    "question": "Welche Rolle spielt der Diskontfaktor $$\\gamma \\in [0,1]$$?",
    "options": [
      "Er begrenzt die Anzahl der Zustände",
      "Er kontrolliert die Varianz der Rewards",
      "Er gewichtet zukünftige Belohnungen",
      "Er bestimmt die Lernrate"
    ],
    "correct_index": 2,
    "explanation": "Der Diskontfaktor legt fest, wie stark zukünftige Rewards im Return berücksichtigt werden."
  },
  {
    "question": "Welche Interpretation hat $$\\gamma = 0$$?",
    "options": [
      "Alle zukünftigen Rewards zählen gleich stark",
      "Nur der unmittelbare Reward zählt",
      "Der Agent maximiert langfristige Belohnung",
      "Der Return divergiert"
    ],
    "correct_index": 1,
    "explanation": "Bei $$\\gamma = 0$$ berücksichtigt der Return nur den nächsten Reward."
  },
  {
    "question": "Welche Aussage zu $$\\gamma = 1$$ ist korrekt?",
    "options": [
      "Der Return ist immer endlich",
      "Nur episodische MDPs sind zulässig",
      "Der Agent ignoriert zukünftige Rewards",
      "Bootstrapping ist nicht möglich"
    ],
    "correct_index": 1,
    "explanation": "Bei $$\\gamma = 1$$ muss das MDP episodisch sein, damit der Return endlich bleibt."
  },
  {
    "question": "Wie ist die Zustandswertfunktion unter einer Policy $$\\pi$$ definiert?",
    "options": [
      "$$V_\\pi(s) = \\mathbb{E}[R_{t+1} \\mid S_t=s]$$",
      "$$V_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t]()
