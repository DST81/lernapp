[
  {
    "question": "Welche Komponente von MENACE entspricht formal am ehesten einer Policy \\(\\pi(a\\mid s)\\) im Reinforcement Learning?",
    "options": [
      "Die Anzahl der Matchboxen",
      "Die Beads-Verteilung innerhalb einer Matchbox",
      "Die Gewinn-/Verluststatistik über alle Spiele",
      "Die Spielregeln von Tic-Tac-Toe"
    ],
    "correct_index": 1,
    "explanation": "Die Beads-Verteilung in einer Matchbox definiert die Wahrscheinlichkeit, eine bestimmte Aktion (Zug) im gegebenen Zustand (Brettkonfiguration) zu wählen, was einer stochastischen Policy entspricht."
  },
  {
    "question": "Warum kann MENACE als tabellarisches Reinforcement-Learning-Verfahren betrachtet werden?",
    "options": [
      "Weil es neuronale Netze zur Generalisierung nutzt",
      "Weil jeder Zustand explizit gespeichert und separat gelernt wird",
      "Weil es kontinuierliche Zustände approximiert",
      "Weil es eine Bellman-Gleichung verwendet"
    ],
    "correct_index": 1,
    "explanation": "Jede Brettkonfiguration besitzt eine eigene Matchbox, d. h. Zustände werden explizit gespeichert ohne Generalisierung – typisch für tabellarische Verfahren."
  },
  {
    "question": "Welche Eigenschaft von MENACE macht es zu einem episodischen Lernproblem?",
    "options": [
      "Die Belohnung ist stochastisch",
      "Die Policy ist deterministisch",
      "Jede Partie endet nach endlich vielen Zügen",
      "Die Umwelt ist vollständig beobachtbar"
    ],
    "correct_index": 2,
    "explanation": "Eine Partie Tic-Tac-Toe endet immer in einem terminalen Zustand (Sieg, Niederlage oder Unentschieden), was das Problem episodisch macht."
  },
  {
    "question": "Welche Art von Feedback verwendet MENACE zur Aktualisierung seiner Policy?",
    "options": [
      "Bootstrapped Targets",
      "Zeitdiskontierte Belohnungen",
      "Evaluatives, verzögertes Feedback",
      "TD-Fehler \\(\\delta_t\\)"
    ],
    "correct_index": 2,
    "explanation": "MENACE erhält erst am Ende eines Spiels Feedback (Gewinn/Verlust/Unentschieden), welches evaluativ und verzögert ist."
  },
  {
    "question": "Warum ist MENACE nicht direkt auf Probleme mit verzögerten Konsequenzen übertragbar?",
    "options": [
      "Weil es keine Exploration erlaubt",
      "Weil Aktionen keinen Einfluss auf zukünftige Zustände haben",
      "Weil es keine Zustände speichert",
      "Weil es zukünftige Konsequenzen einzelner Aktionen nicht explizit bewertet"
    ],
    "correct_index": 3,
    "explanation": "MENACE weist Belohnungen pauschal allen Aktionen einer Episode zu, ohne den spezifischen Einfluss einzelner Aktionen auf spätere Zustände zu modellieren."
  },
  {
    "question": "Welche Aussage beschreibt den Lernmechanismus von MENACE am treffendsten?",
    "options": [
      "Gradientenbasierte Optimierung der Policy",
      "Explizite Maximierung des erwarteten Returns",
      "Heuristische Verstärkung oder Abschwächung beobachteter Aktionen",
      "Dynamische Programmierung mit bekanntem Modell"
    ],
    "correct_index": 2,
    "explanation": "MENACE verstärkt oder schwächt Aktionen heuristisch durch Hinzufügen oder Entfernen von Beads, ohne formale Optimierung oder Modellannahmen."
  },
  {
    "question": "Welche Einschränkung von MENACE folgt direkt aus der vollständigen Zustandsenumeration?",
    "options": [
      "Hohe Varianz der Updates",
      "Schlechte Skalierbarkeit auf große Zustandsräume",
      "Notwendigkeit eines bekannten Modells",
      "Unfähigkeit zu explorieren"
    ],
    "correct_index": 1,
    "explanation": "Da für jeden Zustand eine eigene Matchbox existiert, wächst der Speicherbedarf exponentiell mit der Zustandsgröße."
  },
  {
    "question": "Welche RL-Komponente fehlt MENACE im Vergleich zu modernen RL-Algorithmen vollständig?",
    "options": [
      "Eine Policy",
      "Eine Reward-Funktion",
      "Eine explizite Value-Funktion \\(V(s)\\) oder \\(Q(s,a)\\)",
      "Exploration"
    ],
    "correct_index": 2,
    "explanation": "MENACE speichert keine expliziten Wertfunktionen, sondern passt direkt Aktionswahrscheinlichkeiten an."
  },
  {
    "question": "Warum kann MENACE als Vorläufer von Policy-basierten Methoden interpretiert werden?",
    "options": [
      "Weil es Bellman-Optimalitätsgleichungen nutzt",
      "Weil es Q-Werte approximiert",
      "Weil es direkt eine stochastische Policy lernt",
      "Weil es bootstrapped Targets verwendet"
    ],
    "correct_index": 2,
    "explanation": "MENACE lernt direkt Aktionswahrscheinlichkeiten ohne den Umweg über Wertfunktionen, was dem Grundprinzip von Policy-Gradient-Methoden ähnelt."
  },
  {
    "question": "Welche Annahme über die Umwelt ist für MENACE essenziell?",
    "options": [
      "Partielle Beobachtbarkeit",
      "Kontinuierliche Zustandsräume",
      "Deterministische Übergänge",
      "Vollständige Beobachtbarkeit des Zustands"
    ],
    "correct_index": 3,
    "explanation": "MENACE setzt voraus, dass der aktuelle Spielzustand vollständig bekannt ist, um die passende Matchbox auswählen zu können."
  },
  {
    "question": "In welcher Hinsicht unterscheidet sich MENACE grundlegend von Multi-Armed-Bandit-Problemen?",
    "options": [
      "MENACE hat stochastische Rewards",
      "MENACE besitzt einen Zustandsraum",
      "MENACE maximiert keinen kumulativen Reward",
      "MENACE nutzt keine Exploration"
    ],
    "correct_index": 1,
    "explanation": "Im Gegensatz zu Bandits hängt die Aktionswahl bei MENACE vom aktuellen Zustand (Brettkonfiguration) ab."
  },
  {
    "question": "Welche Zielsetzung verfolgt MENACE implizit?",
    "options": [
      "Minimierung des Regrets",
      "Maximierung des diskontierten Returns \\(G_t = \\sum_k \\gamma^k R_{t+k+1}\\)",
      "Erhöhung der Gewinnwahrscheinlichkeit über viele Episoden",
      "Konvergenz zu einer optimalen Value-Funktion"
    ],
    "correct_index": 2,
    "explanation": "MENACE zielt darauf ab, über wiederholte Spiele häufiger zu gewinnen, ohne explizit diskontierte Returns oder Wertfunktionen zu definieren."
  }
]
