[
  {
    "question": "Welche Komponente von MENACE entspricht formal am ehesten einer Policy \\(\\pi(a\\mid s)\\) im Reinforcement Learning?",
    "options": [
      "Die Anzahl der Matchboxen",
      "Die Beads-Verteilung innerhalb einer Matchbox",
      "Die Gewinn-/Verluststatistik über alle Spiele",
      "Die Spielregeln von Tic-Tac-Toe"
    ],
    "correct_index": 1,
    "explanation": "Die Beads-Verteilung in einer Matchbox definiert die Wahrscheinlichkeit, eine bestimmte Aktion (Zug) im gegebenen Zustand (Brettkonfiguration) zu wählen, was einer stochastischen Policy entspricht."
  },
  {
    "question": "Warum kann MENACE als tabellarisches Reinforcement-Learning-Verfahren betrachtet werden?",
    "options": [
      "Weil es neuronale Netze zur Generalisierung nutzt",
      "Weil jeder Zustand explizit gespeichert und separat gelernt wird",
      "Weil es kontinuierliche Zustände approximiert",
      "Weil es eine Bellman-Gleichung verwendet"
    ],
    "correct_index": 1,
    "explanation": "Jede Brettkonfiguration besitzt eine eigene Matchbox, d. h. Zustände werden explizit gespeichert ohne Generalisierung – typisch für tabellarische Verfahren."
  },
  {
    "question": "Welche Eigenschaft von MENACE macht es zu einem episodischen Lernproblem?",
    "options": [
      "Die Belohnung ist stochastisch",
      "Die Policy ist deterministisch",
      "Jede Partie endet nach endlich vielen Zügen",
      "Die Umwelt ist vollständig beobachtbar"
    ],
    "correct_index": 2,
    "explanation": "Eine Partie Tic-Tac-Toe endet immer in einem terminalen Zustand (Sieg, Niederlage oder Unentschieden), was das Problem episodisch macht."
  },
  {
    "question": "Welche Art von Feedback verwendet MENACE zur Aktualisierung seiner Policy?",
    "options": [
      "Bootstrapped Targets",
      "Zeitdiskontierte Belohnungen",
      "Evaluatives, verzögertes Feedback",
      "TD-Fehler \\(\\delta_t\\)"
    ],
    "correct_index": 2,
    "explanation": "MENACE erhält erst am Ende eines Spiels Feedback (Gewinn/Verlust/Unentschieden), welches evaluativ und verzögert ist."
  },
  {
    "question": "Warum ist MENACE nicht direkt auf Probleme mit verzögerten Konsequenzen übertragbar?",
    "options": [
      "Weil es keine Exploration erlaubt",
      "Weil Aktionen keinen Einfluss auf zukünftige Zustände haben",
      "Weil es keine Zustände speichert",
      "Weil es zukünftige Konsequenzen einzelner Aktionen nicht explizit bewertet"
    ],
    "correct_index": 3,
    "explanation": "MENACE weist Belohnungen pauschal allen Aktionen einer Episode zu, ohne den spezifischen Einfluss einzelner Aktionen auf spätere Zustände zu modellieren."
  },
  {
    "question": "Welche Aussage beschreibt den Lernmechanismus von MENACE am treffendsten?",
    "options": [
      "Gradientenbasierte Optimierung der Policy",
      "Explizite Maximierung des erwarteten Returns",
      "Heuristische Verstärkung oder Abschwächung beobachteter Aktionen",
      "Dynamische Programmierung mit bekanntem Modell"
    ],
    "correct_index": 2,
    "explanation": "MENACE verstärkt oder schwächt Aktionen heuristisch durch Hinzufügen oder Entfernen von Beads, ohne formale Optimierung oder Modellannahmen."
  },
  {
    "question": "Welche Einschränkung von MENACE folgt direkt aus der vollständigen Zustandsenumeration?",
    "options": [
      "Hohe Varianz der Updates",
      "Schlechte Skalierbarkeit auf große Zustandsräume",
      "Notwendigkeit eines bekannten Modells",
      "Unfähigkeit zu explorieren"
    ],
    "correct_index": 1,
    "explanation": "Da für jeden Zustand eine eigene Matchbox existiert, wächst der Speicherbedarf exponentiell mit der Zustandsgröße."
  },
  {
    "question": "Welche RL-Komponente fehlt MENACE im Vergleich zu modernen RL-Algorithmen vollständig?",
    "options": [
      "Eine Policy",
      "Eine Reward-Funktion",
      "Eine explizite Value-Funktion \\(V(s)\\) oder \\(Q(s,a)\\)",
      "Exploration"
    ],
    "correct_index": 2,
    "explanation": "MENACE speichert keine expliziten Wertfunktionen, sondern passt direkt Aktionswahrscheinlichkeiten an."
  },
  {
    "question": "Warum kann MENACE als Vorläufer von Policy-basierten Methoden interpretiert werden?",
    "options": [
      "Weil es Bellman-Optimalitätsgleichungen nutzt",
      "Weil es Q-Werte approximiert",
      "Weil es direkt eine stochastische Policy lernt",
      "Weil es bootstrapped Targets verwendet"
    ],
    "correct_index": 2,
    "explanation": "MENACE lernt direkt Aktionswahrscheinlichkeiten ohne den Umweg über Wertfunktionen, was dem Grundprinzip von Policy-Gradient-Methoden ähnelt."
  },
  {
    "question": "Welche Annahme über die Umwelt ist für MENACE essenziell?",
    "options": [
      "Partielle Beobachtbarkeit",
      "Kontinuierliche Zustandsräume",
      "Deterministische Übergänge",
      "Vollständige Beobachtbarkeit des Zustands"
    ],
    "correct_index": 3,
    "explanation": "MENACE setzt voraus, dass der aktuelle Spielzustand vollständig bekannt ist, um die passende Matchbox auswählen zu können."
  },
  {
    "question": "In welcher Hinsicht unterscheidet sich MENACE grundlegend von Multi-Armed-Bandit-Problemen?",
    "options": [
      "MENACE hat stochastische Rewards",
      "MENACE besitzt einen Zustandsraum",
      "MENACE maximiert keinen kumulativen Reward",
      "MENACE nutzt keine Exploration"
    ],
    "correct_index": 1,
    "explanation": "Im Gegensatz zu Bandits hängt die Aktionswahl bei MENACE vom aktuellen Zustand (Brettkonfiguration) ab."
  },
  {
    "question": "Welche Zielsetzung verfolgt MENACE implizit?",
    "options": [
      "Minimierung des Regrets",
      "Maximierung des diskontierten Returns \\(G_t = \\sum_k \\gamma^k R_{t+k+1}\\)",
      "Erhöhung der Gewinnwahrscheinlichkeit über viele Episoden",
      "Konvergenz zu einer optimalen Value-Funktion"
    ],
    "correct_index": 2,
    "explanation": "MENACE zielt darauf ab, über wiederholte Spiele häufiger zu gewinnen, ohne explizit diskontierte Returns oder Wertfunktionen zu definieren."
  },
  {
    "question": "Welche Annahme unterscheidet das Multi-Armed-Bandit-Problem fundamental von einem MDP?",
    "options": [
      "Die Reward-Verteilungen sind unbekannt",
      "Aktionen beeinflussen zukünftige Zustände",
      "Es gibt keinen Zustandsraum",
      "Die Rewards sind verrauscht"
    ],
    "correct_index": 2,
    "explanation": "Bandits besitzen keinen Zustandsraum; jede Aktion ist unabhängig von vorherigen Entscheidungen."
  },
  {
    "question": "Was ist das Optimierungsziel im klassischen K-armed-Bandit-Problem?",
    "options": [
      "Maximierung des diskontierten Returns \\(G_t = \\sum_k \\gamma^k R_{t+k+1}\\)",
      "Minimierung der Varianz der Rewards",
      "Maximierung der kumulativen Belohnung \\(\\sum_{t=1}^T r_t\\)",
      "Konvergenz zu einer optimalen Policy"
    ],
    "correct_index": 2,
    "explanation": "Bandits maximieren die kumulative Belohnung über einen endlichen Horizont T, ohne Diskontierung."
  },
  {
    "question": "Welche Aussage beschreibt das Exploration–Exploitation-Dilemma korrekt?",
    "options": [
      "Exploration erhöht kurzfristig den Reward",
      "Exploitation reduziert langfristig den Regret",
      "Exploration sammelt Information, Exploitation nutzt aktuelles Wissen",
      "Exploitation ist nur bei stationären Rewards sinnvoll"
    ],
    "correct_index": 2,
    "explanation": "Exploration dient dem Lernen über unbekannte Aktionen, Exploitation nutzt die aktuell beste Schätzung."
  },
  {
    "question": "Wie ist die Entscheidungsregel einer \\(\\varepsilon\\)-greedy-Strategie definiert?",
    "options": [
      "Wähle zufällig mit Wahrscheinlichkeit \\(1-\\varepsilon\\)",
      "Wähle \\(\\arg\\max_a Q_t(a)\\) mit Wahrscheinlichkeit \\(1-\\varepsilon\\)",
      "Wähle immer die Aktion mit maximalem UCB-Wert",
      "Ziehe jede Aktion gleich häufig"
    ],
    "correct_index": 1,
    "explanation": "Mit Wahrscheinlichkeit \\(1-\\varepsilon\\) wird die aktuell beste Aktion gewählt, sonst zufällig exploriert."
  },
  {
    "question": "Welche Update-Regel ist für stationäre Reward-Verteilungen geeignet?",
    "options": [
      "Konstante Schrittweite \\(\\alpha\\)",
      "Exponentiell gewichteter Mittelwert",
      "Sample-Average-Update",
      "TD-Update mit Bootstrapping"
    ],
    "correct_index": 2,
    "explanation": "Bei stationären Rewards konvergiert das Sample-Average-Update gegen den wahren Erwartungswert."
  },
  {
    "question": "Wie lautet das Sample-Average-Update für den Aktionswert?",
    "options": [
      "\\(Q_{n+1}(a) = Q_n(a) + \\alpha (R_n - Q_n(a))\\)",
      "\\(Q_{n+1}(a) = \\max(Q_n(a), R_n)\\)",
      "\\(Q_{n+1}(a) = Q_n(a) + \\frac{1}{N_n(a)} (R_n - Q_n(a))\\)",
      "\\(Q_{n+1}(a) = R_n\\)"
    ],
    "correct_index": 2,
    "explanation": "Das Sample-Average-Update nutzt eine adaptive Schrittweite \\(1/N_n(a)\\)."
  },
  {
    "question": "Warum ist bei nicht-stationären Bandits eine konstante Schrittweite \\(\\alpha\\) vorteilhaft?",
    "options": [
      "Sie eliminiert Exploration",
      "Sie vergisst alte Beobachtungen schneller",
      "Sie reduziert den Rechenaufwand",
      "Sie macht das Verfahren unbiased"
    ],
    "correct_index": 1,
    "explanation": "Eine konstante Schrittweite gewichtet neue Rewards stärker und passt sich Drift an."
  },
  {
    "question": "Welche Idee steckt hinter optimistischen Initialwerten?",
    "options": [
      "Reduktion der Varianz der Reward-Schätzung",
      "Exploration durch anfänglich überschätzte Q-Werte",
      "Beschleunigung der Konvergenz bei UCB",
      "Vermeidung von Regret"
    ],
    "correct_index": 1,
    "explanation": "Hohe Startwerte zwingen den Agenten, alle Aktionen mindestens einmal auszuprobieren."
  },
  {
    "question": "Welche Entscheidungsregel definiert UCB(c)?",
    "options": [
      "\\(a_t = \\arg\\max_a Q_t(a)\\)",
      "\\(a_t = \\arg\\max_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]\\)",
      "\\(a_t = \\arg\\max_a \\frac{Q_t(a)}{N_t(a)}\\)",
      "\\(a_t = \\arg\\min_a N_t(a)\\)"
    ],
    "correct_index": 1,
    "explanation": "UCB kombiniert Mittelwertschätzung mit einem Unsicherheitsbonus."
  },
  {
    "question": "Welche Rolle spielt der Parameter \\(c\\) in UCB?",
    "options": [
      "Diskontierungsfaktor",
      "Schrittweite des Updates",
      "Stärke der Exploration",
      "Anzahl der Arme"
    ],
    "correct_index": 2,
    "explanation": "Ein größeres \\(c\\) erhöht den Explorationsbonus für selten gezogene Arme."
  },
  {
    "question": "Warum muss bei UCB jeder Arm mindestens einmal gezogen werden?",
    "options": [
      "Um Konvergenz zu garantieren",
      "Um Division durch Null zu vermeiden",
      "Um stationäre Rewards sicherzustellen",
      "Um Regret zu minimieren"
    ],
    "correct_index": 1,
    "explanation": "Der Term \\(N_t(a)\\) steht im Nenner des Unsicherheitsbonus."
  },
  {
    "question": "Was misst der kumulative Regret \\(R_T\\) im stationären Bandit?",
    "options": [
      "Die Varianz der Rewards",
      "Die Differenz zwischen optimalem und erzieltem Gesamtreward",
      "Die Anzahl suboptimaler Aktionen",
      "Den durchschnittlichen Reward"
    ],
    "correct_index": 1,
    "explanation": "Regret vergleicht die erzielte Belohnung mit der hypothetischen Belohnung des besten Arms."
  },
  {
    "question": "Wie ist der kumulative Regret formal definiert?",
    "options": [
      "\\(R_T = \\sum_{t=1}^T r_t\\)",
      "\\(R_T = \\sum_{t=1}^T (\\mu^* - r_t)\\)",
      "\\(R_T = \\max_a Q_T(a)\\)",
      "\\(R_T = \\sum_a N_T(a)\\)"
    ],
    "correct_index": 1,
    "explanation": "\\(\\mu^*\\) ist der Erwartungswert des optimalen Arms; Regret misst entgangenen Reward."
  },
  {
    "question": "Welche Aussage trifft auf \\(\\varepsilon\\)-greedy im Vergleich zu UCB zu?",
    "options": [
      "ε-greedy nutzt gerichtete Exploration",
      "UCB exploriert unabhängig von Unsicherheit",
      "ε-greedy exploriert zufällig, UCB systematisch",
      "Beide sind identisch bei \\(\\varepsilon = 0\\)"
    ],
    "correct_index": 2,
    "explanation": "ε-greedy exploriert zufällig, während UCB Unsicherheit explizit berücksichtigt."
  },
  {
    "question": "Wann ist UCB gegenüber \\(\\varepsilon\\)-greedy typischerweise überlegen?",
    "options": [
      "Bei stark nicht-stationären Rewards",
      "Bei sehr großen Zustandsräumen",
      "Bei stationären Bandits mit begrenztem Horizont",
      "Wenn Exploration vollständig vermieden werden soll"
    ],
    "correct_index": 2,
    "explanation": "UCB erzielt bei stationären Problemen oft logarithmischen Regret durch gezielte Exploration."
  }
]

