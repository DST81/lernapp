[
  {
    "question": "Welche Komponente von MENACE entspricht formal am ehesten einer Policy $$\\pi(a \\mid s)$$ im Reinforcement Learning?",
    "options": [
      "Die Anzahl der Matchboxen",
      "Die Beads-Verteilung innerhalb einer Matchbox",
      "Die Gewinn-/Verluststatistik über alle Spiele",
      "Die Spielregeln von Tic-Tac-Toe"
    ],
    "correct_index": 1,
    "explanation": "Die Beads-Verteilung in einer Matchbox definiert die Wahrscheinlichkeit, eine bestimmte Aktion $$a$$ im gegebenen Zustand $$s$$ zu wählen, was einer stochastischen Policy $$\\pi(a \\mid s)$$ entspricht."
  },
  {
    "question": "Welche Art von Feedback verwendet MENACE zur Aktualisierung seiner Policy?",
    "options": [
      "Bootstrapped Targets",
      "Zeitdiskontierte Belohnungen",
      "Evaluatives, verzögertes Feedback",
      "TD-Fehler $$\\delta_t$$"
    ],
    "correct_index": 2,
    "explanation": "MENACE erhält erst am Ende eines Spiels Feedback (Gewinn/Verlust/Unentschieden), welches evaluativ und verzögert ist."
  },
  {
    "question": "Welche RL-Komponente fehlt MENACE im Vergleich zu modernen RL-Algorithmen vollständig?",
    "options": [
      "Eine Policy",
      "Eine Reward-Funktion",
      "Eine explizite Value-Funktion $$V(s)$$ oder $$Q(s,a)$$",
      "Exploration"
    ],
    "correct_index": 2,
    "explanation": "MENACE speichert keine expliziten Wertfunktionen $$V(s)$$ oder $$Q(s,a)$$, sondern passt direkt Aktionswahrscheinlichkeiten an."
  },
  {
    "question": "Welche Zielsetzung verfolgt MENACE implizit?",
    "options": [
      "Minimierung des Regrets",
      "Maximierung des diskontierten Returns $$G_t = \\sum_k \\gamma^k R_{t+k+1}$$",
      "Erhöhung der Gewinnwahrscheinlichkeit über viele Episoden",
      "Konvergenz zu einer optimalen Value-Funktion"
    ],
    "correct_index": 2,
    "explanation": "MENACE zielt darauf ab, über wiederholte Spiele häufiger zu gewinnen, ohne explizit diskontierte Returns $$G_t$$ oder Wertfunktionen zu definieren."
  },
  {
    "question": "Wie ist die Entscheidungsregel einer $$\\varepsilon$$-greedy-Strategie definiert?",
    "options": [
      "Wähle zufällig mit Wahrscheinlichkeit $$1-\\varepsilon$$",
      "Wähle $$\\arg\\max_a Q_t(a)$$ mit Wahrscheinlichkeit $$1-\\varepsilon$$",
      "Wähle immer die Aktion mit maximalem UCB-Wert",
      "Ziehe jede Aktion gleich häufig"
    ],
    "correct_index": 1,
    "explanation": "Mit Wahrscheinlichkeit $$1-\\varepsilon$$ wird die aktuell beste Aktion $$\\arg\\max_a Q_t(a)$$ gewählt, sonst zufällig exploriert."
  },
  {
    "question": "Wie lautet das Sample-Average-Update für den Aktionswert?",
    "options": [
      "$$Q_{n+1}(a) = Q_n(a) + \\alpha (R_n - Q_n(a))$$",
      "$$Q_{n+1}(a) = \\max(Q_n(a), R_n)$$",
      "$$Q_{n+1}(a) = Q_n(a) + \\frac{1}{N_n(a)} (R_n - Q_n(a))$$",
      "$$Q_{n+1}(a) = R_n$$"
    ],
    "correct_index": 2,
    "explanation": "Das Sample-Average-Update nutzt eine adaptive Schrittweite $$\\frac{1}{N_n(a)}$$."
  },
  {
    "question": "Welche Entscheidungsregel definiert UCB?",
    "options": [
      "$$a_t = \\arg\\max_a Q_t(a)$$",
      "$$a_t = \\arg\\max_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$",
      "$$a_t = \\arg\\max_a \\frac{Q_t(a)}{N_t(a)}$$",
      "$$a_t = \\arg\\min_a N_t(a)$$"
    ],
    "correct_index": 1,
    "explanation": "UCB kombiniert Mittelwertschätzung $$Q_t(a)$$ mit einem Unsicherheitsbonus."
  },
  {
    "question": "Wie ist der kumulative Regret formal definiert?",
    "options": [
      "$$R_T = \\sum_{t=1}^T r_t$$",
      "$$R_T = \\sum_{t=1}^T (\\mu^* - r_t)$$",
      "$$R_T = \\max_a Q_T(a)$$",
      "$$R_T = \\sum_a N_T(a)$$"
    ],
    "correct_index": 1,
    "explanation": "Der kumulative Regret $$R_T$$ misst den entgangenen Reward relativ zum optimalen Erwartungswert $$\\mu^*$$."
  }
]
