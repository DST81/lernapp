[
    {
    "question": "Welche Auswirkung hat das Hinzufügen von Dropout in einem neuronalen Netz während des Trainings?",
    "options": [
      "Es beschleunigt das Training",
      "Es erhöht die Modellkomplexität",
      "Es reduziert Overfitting durch zufälliges Deaktivieren von Neuronen",
      "Es erhöht die Anzahl der Parameter im Modell"
    ],
    "correct_index": 2,
    "explanation": "Dropout ist eine Regularisierungsmethode, die Overfitting reduziert, indem zufällig Neuronen während des Trainings deaktiviert werden."
  },
  {
    "question": "Warum ist die Initialisierung der Gewichte bei tiefen Netzen besonders wichtig?",
    "options": [
      "Weil sie das Modell robuster gegen Angriffe macht",
      "Weil falsche Initialisierung den Gradientenfluss behindern kann",
      "Weil Initialisierung das Overfitting erhöht",
      "Weil alle Gewichte null sein sollten"
    ],
    "correct_index": 1,
    "explanation": "Eine schlechte Initialisierung kann zu verschwindenden oder explodierenden Gradienten führen, was das Training behindert."
  },
  {
    "question": "Wozu dient der Mechanismus der 'Attention' in einem Transformer-Modell?",
    "options": [
      "Er ersetzt vollständig die Aktivierungsfunktionen",
      "Er erlaubt es dem Modell, relevante Teile der Eingabe gezielt zu gewichten",
      "Er verringert die Anzahl der Modellparameter",
      "Er entfernt das Bedürfnis nach Backpropagation"
    ],
    "correct_index": 1,
    "explanation": "Attention gewichtet die Wichtigkeit verschiedener Eingabeteile kontextabhängig und verbessert damit die Verarbeitung von Sequenzdaten."
  },
  {
    "question": "Welche Funktion erfüllt die Positionsembedding-Komponente im Transformer?",
    "options": [
      "Sie reduziert die Modellgröße",
      "Sie gibt dem Modell Information über die Reihenfolge der Token",
      "Sie verhindert Überanpassung",
      "Sie ersetzt den Self-Attention-Mechanismus"
    ],
    "correct_index": 1,
    "explanation": "Da Transformer keine rekursive oder sequentielle Verarbeitung haben, werden Positionsinformationen durch Positionsembeddings hinzugefügt."
  },
  {
    "question": "Was unterscheidet LSTMs von klassischen RNNs?",
    "options": [
      "LSTMs verwenden kein Backpropagation",
      "LSTMs haben eine zusätzliche Speicherzelle zur besseren Langzeitabhängigkeit",
      "LSTMs sind nicht differenzierbar",
      "LSTMs benötigen keine Trainingsdaten"
    ],
    "correct_index": 1,
    "explanation": "LSTMs verfügen über eine Speicherzelle mit Gate-Mechanismen, um Langzeitinformationen besser zu speichern und weiterzugeben."
  },
  {
    "question": "Welche Herausforderung adressiert Batch Normalization in tiefen neuronalen Netzen?",
    "options": [
      "Explodierende Gradienten",
      "Vanishing Gradients",
      "Internal Covariate Shift",
      "Overfitting durch zu große Modelle"
    ],
    "correct_index": 2,
    "explanation": "Batch Normalization reduziert den 'Internal Covariate Shift' und stabilisiert so das Training tiefer Netze."
  },
  {
    "question": "Was beschreibt der Begriff 'gradient clipping' in RNNs?",
    "options": [
      "Das Abschalten von Neuronen mit kleinem Gradienten",
      "Das Limitieren der Gradienten, um Explodieren zu verhindern",
      "Das Anwenden von Dropout",
      "Das Entfernen aller negativen Gradienten"
    ],
    "correct_index": 1,
    "explanation": "Gradient Clipping verhindert zu große Updates bei explodierenden Gradienten und stabilisiert das Training, besonders bei RNNs."
  },
  {
    "question": "Wie verändert Data Augmentation typischerweise den Trainingsprozess bei CNNs?",
    "options": [
      "Es macht das Modell kleiner",
      "Es erhöht die Genauigkeit durch synthetisch erzeugte Varianz",
      "Es verhindert die Konvergenz",
      "Es reduziert die Trainingsdatenmenge"
    ],
    "correct_index": 1,
    "explanation": "Durch Data Augmentation entsteht mehr Vielfalt in den Trainingsdaten, was das Modell robuster macht und Overfitting reduziert."
  },
  {
    "question": "Warum sind 1D-CNNs für Sequenzdaten effizienter als RNNs?",
    "options": [
      "Weil sie die Reihenfolge ignorieren",
      "Weil sie keine Backpropagation benötigen",
      "Weil sie lokal auf Muster achten und parallelisiert werden können",
      "Weil sie keine Gewichtsmatrizen verwenden"
    ],
    "correct_index": 2,
    "explanation": "1D-CNNs erkennen lokale Muster und sind durch ihre Parallelisierbarkeit oft schneller und ressourcenschonender als RNNs."
  },
  {
    "question": "Was ist ein typisches Anzeichen von Overfitting während des Trainings?",
    "options": [
      "Die Trainings- und Validierungsfehler sinken gleichermaßen",
      "Der Trainingsfehler sinkt, aber der Validierungsfehler steigt",
      "Beide Fehler steigen",
      "Die Genauigkeit auf beiden Sets steigt"
    ],
    "correct_index": 1,
    "explanation": "Ein sinkender Trainingsfehler bei steigendem Validierungsfehler ist ein klassisches Zeichen für Overfitting."
  },
  {
    "question": "Welche Rolle spielt der Loss-Function bei Supervised Learning?",
    "options": [
      "Sie berechnet die Metriken auf dem Testset",
      "Sie hilft, die optimalen Hyperparameter zu bestimmen",
      "Sie quantifiziert den Fehler zwischen Prediction und Ground Truth",
      "Sie bestimmt die Netzwerkstruktur"
    ],
    "correct_index": 2,
    "explanation": "Die Loss-Funktion misst, wie stark die Vorhersagen vom wahren Label abweichen – sie ist zentral für das Lernen."
  },
  {
    "question": "Was bewirkt ein kleiner Lernratenwert beim Training eines Modells?",
    "options": [
      "Das Modell lernt schneller",
      "Das Modell überspringt lokale Minima",
      "Das Modell konvergiert langsam, aber stabiler",
      "Das Modell wird untrainierbar"
    ],
    "correct_index": 2,
    "explanation": "Eine kleine Lernrate sorgt für langsamere, aber stabilere Schritte im Gradientenabstieg."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz?",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells?",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen?",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN?",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Worin liegt die Stärke von 1D-CNNs bei Textklassifikationsaufgaben?",
    "options": [
      "Sie sind unabhängig von der Wortreihenfolge",
      "Sie erkennen lokale Muster in Textsequenzen effizient und parallelisierbar",
      "Sie modellieren explizit Syntaxbäume",
      "Sie berechnen Wahrscheinlichkeiten für Tokens"
    ],
    "correct_index": 1,
    "explanation": "1D-CNNs erkennen lokale Muster wie N-Gramme effizient und eignen sich besonders für Textklassifikation."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (CNN)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Deep Clustering)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (CNN) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Multi-Input/Output)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (RNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Deep Clustering) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells? (Regularisierung)",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Multi-Input/Output)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung) (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Deep Clustering) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Transformers)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Optimierung) (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Grundlagen) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Deep Clustering) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Deep Clustering) (Grundlagen) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells? (Regularisierung) (Optimierung)",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Grundlagen) (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Grundlagen) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (Regularisierung) (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Transformers) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung) (Deep Clustering)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (Regularisierung) (Regularisierung) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Grundlagen) (Regularisierung) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Regularisierung) (Optimierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Grundlagen) (CNN) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Transformers) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Deep Clustering) (Multi-Input/Output)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Deep Clustering) (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Deep Clustering) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Grundlagen) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Transformers) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Multi-Input/Output)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Transformers) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Transformers) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Worin liegt die Stärke von 1D-CNNs bei Textklassifikationsaufgaben? (Optimierung)",
    "options": [
      "Sie sind unabhängig von der Wortreihenfolge",
      "Sie erkennen lokale Muster in Textsequenzen effizient und parallelisierbar",
      "Sie modellieren explizit Syntaxbäume",
      "Sie berechnen Wahrscheinlichkeiten für Tokens"
    ],
    "correct_index": 1,
    "explanation": "1D-CNNs erkennen lokale Muster wie N-Gramme effizient und eignen sich besonders für Textklassifikation."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Regularisierung) (Optimierung) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells? (Regularisierung) (Multi-Input/Output)",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung) (Deep Clustering)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (RNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Optimierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (Regularisierung) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (RNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Optimierung) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Multi-Input/Output)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  }
]
