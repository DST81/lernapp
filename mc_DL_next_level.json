[
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz?",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells?",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen?",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN?",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Worin liegt die Stärke von 1D-CNNs bei Textklassifikationsaufgaben?",
    "options": [
      "Sie sind unabhängig von der Wortreihenfolge",
      "Sie erkennen lokale Muster in Textsequenzen effizient und parallelisierbar",
      "Sie modellieren explizit Syntaxbäume",
      "Sie berechnen Wahrscheinlichkeiten für Tokens"
    ],
    "correct_index": 1,
    "explanation": "1D-CNNs erkennen lokale Muster wie N-Gramme effizient und eignen sich besonders für Textklassifikation."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (CNN)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Deep Clustering)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (CNN) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Multi-Input/Output)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (RNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Deep Clustering) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells? (Regularisierung)",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Multi-Input/Output)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung) (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Deep Clustering) (Grundlagen)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Transformers)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Optimierung) (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Grundlagen) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Deep Clustering) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Deep Clustering) (Grundlagen) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells? (Regularisierung) (Optimierung)",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Grundlagen) (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Grundlagen) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (Regularisierung) (Regularisierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Transformers) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung) (Deep Clustering)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Transformers) (Regularisierung) (Regularisierung) (Optimierung)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Grundlagen) (Regularisierung) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Deep Clustering) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Regularisierung) (Optimierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Deep Clustering)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Grundlagen) (CNN) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Transformers) (Regularisierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Deep Clustering) (Multi-Input/Output)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Deep Clustering) (CNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (CNN) (Regularisierung) (Deep Clustering) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Grundlagen) (Transformers)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Transformers) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (CNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Transformers) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Multi-Input/Output)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Transformers) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Transformers) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Worin liegt die Stärke von 1D-CNNs bei Textklassifikationsaufgaben? (Optimierung)",
    "options": [
      "Sie sind unabhängig von der Wortreihenfolge",
      "Sie erkennen lokale Muster in Textsequenzen effizient und parallelisierbar",
      "Sie modellieren explizit Syntaxbäume",
      "Sie berechnen Wahrscheinlichkeiten für Tokens"
    ],
    "correct_index": 1,
    "explanation": "1D-CNNs erkennen lokale Muster wie N-Gramme effizient und eignen sich besonders für Textklassifikation."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Regularisierung) (Optimierung) (Transformers)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Grundlagen) (Regularisierung) (Deep Clustering)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Warum führt eine zu hohe Lernrate oft zu divergierendem Verhalten beim Training eines Modells? (Regularisierung) (Multi-Input/Output)",
    "options": [
      "Weil das Modell dann zu wenig reguliert wird",
      "Weil die Gewichtsanpassungen zu klein ausfallen",
      "Weil der Fehler nicht propagiert werden kann",
      "Weil die Gewichtsanpassungen zu stark sind und das Minimum übersprungen wird"
    ],
    "correct_index": 3,
    "explanation": "Eine zu hohe Lernrate führt zu großen Gewichtsänderungen, die das Minimum der Fehlerfunktion oft überspringen und das Training instabil machen."
  },
  {
    "question": "Was ist der Hauptunterschied zwischen LSTM und einem einfachen RNN? (Regularisierung) (Multi-Input/Output) (Optimierung) (Deep Clustering)",
    "options": [
      "LSTM nutzt kein Backpropagation Through Time (BPTT)",
      "LSTM kann keine Langzeitabhängigkeiten verarbeiten",
      "LSTM verwendet Gating-Mechanismen, um Informationen selektiv weiterzugeben",
      "LSTM hat keine rekursive Struktur"
    ],
    "correct_index": 2,
    "explanation": "LSTM-Netze verwenden spezielle Gating-Mechanismen, um Informationen über längere Zeiträume zu speichern und selektiv zu vergessen."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (RNN)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (RNN) (Deep Clustering) (CNN) (Optimierung)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Welche Funktion hat der Backpropagation-Algorithmus in einem neuronalen Netz? (Deep Clustering) (Grundlagen) (Regularisierung) (RNN)",
    "options": [
      "Er initialisiert die Gewichte zufällig",
      "Er bestimmt die Anzahl der Neuronen in der Ausgabeschicht",
      "Er aktualisiert die Gewichte durch Rückführung des Fehlers",
      "Er entfernt redundante Neuronen aus dem Netz"
    ],
    "correct_index": 2,
    "explanation": "Backpropagation berechnet den Gradienten des Fehlers und aktualisiert die Gewichte durch Rückführung des Fehlers entlang des Netzwerks."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (RNN) (Grundlagen)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (Optimierung) (Optimierung)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  },
  {
    "question": "Was ist ein Vorteil von ReLU gegenüber Sigmoid in tiefen neuronalen Netzen? (CNN) (Multi-Input/Output)",
    "options": [
      "ReLU ist differenzierbar an allen Punkten",
      "ReLU vermeidet das Vanishing Gradient Problem besser als Sigmoid",
      "ReLU hat eine nichtlineare Aktivierungsfunktion mit Begrenzung",
      "ReLU funktioniert nur für Klassifikationsaufgaben"
    ],
    "correct_index": 1,
    "explanation": "ReLU ist in tiefen Netzen effizienter, da es im Gegensatz zu Sigmoid nicht unter dem Vanishing Gradient Problem leidet."
  }
]
